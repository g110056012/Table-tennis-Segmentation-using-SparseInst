{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ea49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "from torch.utils.data import random_split\n",
    "# set the numpy seed for better reproducibility\n",
    "import numpy as np\n",
    "# import the necessary packages\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import ToTensor\n",
    "import argparse\n",
    "import imutils\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cecab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd97aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7660, 12, 256, 256)\n",
      "(7660, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the saved numpy arrays\n",
    "np_train_data = np.load('/home/yoson/SparseInst/official/SparseInst/table-tennis/pose_data/256_pose_train_data_allframe.npy')\n",
    "np_train_label = np.load('/home/yoson/SparseInst/official/SparseInst/table-tennis/pose_data/256_pose_train_label_allframe.npy')\n",
    "print(np_train_data.shape)\n",
    "print(np_train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c28f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310a9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from matplotlib import cm\n",
    "\n",
    "# for index, sequence in  enumerate(np_train_data):\n",
    "#     for img_index, image in enumerate(sequence):\n",
    "#         np_train_data[index][img_index] = Image.fromarray(image, mode='L')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87296ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890060d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_train_data = np.asarray(np_train_data).astype(np.float32()) / 255.0\n",
    "# print(np_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf778f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7307260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf027cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np_train_data_reshape = np_train_data\n",
    "\n",
    "train_data_list = []\n",
    "# test_data_list = []\n",
    "for i in range(len(np_train_data_reshape)):\n",
    "#     print(np_train_data_reshape[i].shape())\n",
    "    train_data_list.append([np_train_data_reshape[i], np_train_label[i]])\n",
    "    \n",
    "    \n",
    "dataset = train_data_list\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "i1, l1 = next(iter(train_dataset))\n",
    "# print(np.shape((train_dataset)), np.shape(val_dataset))\n",
    "print(np.shape(train_dataset))\n",
    "print(np.shape(train_dataset[0][0]))\n",
    "# print(train_dataset[0][0])\n",
    "\n",
    "\n",
    "# Split test\n",
    "train_size2 = int(0.9 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size2\n",
    "train_dataset, test_dataset = random_split(train_dataset, [train_size2, test_size])\n",
    "i1, l1 = next(iter(train_dataset))\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "print(i1.shape)\n",
    "\n",
    "\n",
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "# trainDataLoader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "testDataLoader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "# trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "# valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "valSteps = len(val_dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff581c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd02877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7660, 1, 12, 256, 256)\n",
      "5515 1532 613\n",
      "(1, 12, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset in order\n",
    "# 3D\n",
    "reshape_np_train_data = np.expand_dims(np_train_data, axis=1)\n",
    "print(reshape_np_train_data.shape)\n",
    "np_train_data_reshape = reshape_np_train_data\n",
    "\n",
    "train_data_list = []\n",
    "# test_data_list = []\n",
    "for i in range(len(np_train_data_reshape)):\n",
    "#     print(np_train_data_reshape[i].shape())\n",
    "    train_data_list.append([np_train_data_reshape[i], np_train_label[i]])\n",
    "\n",
    "dataset = train_data_list\n",
    "split_index = int(0.8 * len(dataset))\n",
    "train_dataset_temp = dataset[:split_index]\n",
    "val_dataset = dataset[split_index:]\n",
    "\n",
    "split_index_2 = int(0.9 * len(train_dataset_temp))\n",
    "train_dataset = train_dataset_temp[:split_index_2]\n",
    "test_dataset = train_dataset_temp[split_index_2:] \n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "print(np.shape(train_dataset[0][0]))\n",
    "\n",
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "valDataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testDataLoader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "# trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "# valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "valSteps = len(val_dataset) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3023aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   4   8  10  12  16  18  20  24  26  28  31  32  36  39  40  41  42\n",
      "  44  45  47  48  49  50  51  52  53  55  56  57  58  59  60  61  63  64\n",
      "  65  66  67  68  69  71  72  73  75  76  77  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 121 122\n",
      " 123 124 125 126 128 129 130 131 132 133 134 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 160 161\n",
      " 162 163 165 166 167 168 169 171 172 173 174 175 177 179 180 181 183 184\n",
      " 185 186 187 188 189 192 193 194 196 197 199 200 202 203 204 205 208 210\n",
      " 211 212 214 216 218 220 222 224 228 230 232 235 236 243 245 249 255]\n"
     ]
    }
   ],
   "source": [
    "for x, y in trainDataLoader:\n",
    "    print(np.unique(x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5799805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0606d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847316b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9d359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5da9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D\n",
    "reshape_np_train_data = np.expand_dims(np_train_data, axis=1)\n",
    "print(reshape_np_train_data.shape)\n",
    "np_train_data_reshape = reshape_np_train_data\n",
    "\n",
    "train_data_list = []\n",
    "# test_data_list = []\n",
    "for i in range(len(np_train_data_reshape)):\n",
    "#     print(np_train_data_reshape[i].shape())\n",
    "    train_data_list.append([np_train_data_reshape[i], np_train_label[i]])\n",
    "    \n",
    "    \n",
    "dataset = train_data_list\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "i1, l1 = next(iter(train_dataset))\n",
    "# print(np.shape((train_dataset)), np.shape(val_dataset))\n",
    "# print(train_dataset.shape)\n",
    "\n",
    "# reshape_train_dataset = np.expand_dims(train_dataset, axis=0)\n",
    "# print(np.shape(reshape_train_dataset))\n",
    "print('------')\n",
    "print(train_dataset[0][0].shape)\n",
    "# print(train_dataset[0][0])\n",
    "\n",
    "\n",
    "# Split test\n",
    "train_size2 = int(0.9 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size2\n",
    "train_dataset, test_dataset = random_split(train_dataset, [train_size2, test_size])\n",
    "i1, l1 = next(iter(train_dataset))\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "print(i1.shape)\n",
    "\n",
    "\n",
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "# trainDataLoader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "testDataLoader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "# trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "# valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "valSteps = len(val_dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8e92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73918f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 256X256\n",
    "# class Conv2DNet(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(Conv2DNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.dropout = nn.Dropout2d(p=0.2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(256 * 64 * 64, 64)       \n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(64, num_classes)\n",
    "#         # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu4(x)\n",
    "#         x = self.fc2(x)\n",
    "#         # x = self.softmax(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # 640X640    \n",
    "# class Conv2DNet(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(Conv2DNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.dropout = nn.Dropout2d(p=0.2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(256 * 160 * 160, 64)       \n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu4(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "        \n",
    "# 512X512    \n",
    "class Conv2DNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Conv2DNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 128 * 128, 64) \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98df1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 12 * 32 * 32, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "#         print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# # Create an instance of the model\n",
    "# num_classes = 5\n",
    "# model = Conv3DNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5814714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ede96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d8d9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the Conv3DNet model...\n",
      "Conv3DNet(\n",
      "  (conv1): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (maxpool3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=786432, out_features=64, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the Conv3DNet model\n",
    "print(\"[INFO] initializing the Conv3DNet model...\")\n",
    "num_classes = 5\n",
    "model = Conv3DNet(num_classes)\n",
    "# model = Conv2DNet(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ec6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "model = model.cuda()\n",
    "# model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846379f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc15195e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 1/10: [INFO] EPOCH: 1/10\n",
      "Train loss: 1.171513, Train accuracy: 0.9088\n",
      "Val loss: 0.274510, Val accuracy: 0.9262\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 2/10: [INFO] EPOCH: 2/10\n",
      "Train loss: 0.024876, Train accuracy: 0.9944\n",
      "Val loss: 0.391466, Val accuracy: 0.9282\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 3/10: [INFO] EPOCH: 3/10\n",
      "Train loss: 0.002186, Train accuracy: 0.9980\n",
      "Val loss: 0.487892, Val accuracy: 0.9210\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 4/10: [INFO] EPOCH: 4/10\n",
      "Train loss: 0.000591, Train accuracy: 0.9980\n",
      "Val loss: 0.537948, Val accuracy: 0.9236\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 5/10: [INFO] EPOCH: 5/10\n",
      "Train loss: 0.000271, Train accuracy: 0.9980\n",
      "Val loss: 0.571993, Val accuracy: 0.9210\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 6/10: [INFO] EPOCH: 6/10\n",
      "Train loss: 0.000174, Train accuracy: 0.9980\n",
      "Val loss: 0.605019, Val accuracy: 0.9204\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch 7/10: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# add the loss to the total training loss so far and calculate the number of correct predictions\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     totalTrainLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m---> 43\u001b[0m     trainCorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# switch off autograd for evaluation\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# set the model in evaluation mode\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "# loop over our epochs\n",
    "for idx, e in enumerate(range(0, EPOCHS)):\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    print(f'Epoch {e + 1}/{EPOCHS}:', end = ' ')\n",
    "\n",
    "    # set the model in training mode\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize the number of correct predictions in the training and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for x, y in trainDataLoader:\n",
    "        # send the input to the device\n",
    "#         (x, y) = (x.to(device), y.to(device))\n",
    "        x = x.float().cuda(non_blocking = True)\n",
    "        y = y.view(-1).cuda(non_blocking = True)\n",
    "        \n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        # print(pred)\n",
    "        # print('--------------------------')\n",
    "        # print(labels)\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the loss to the total training loss so far and calculate the number of correct predictions\n",
    "        totalTrainLoss += loss\n",
    "        trainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "    # switch off autograd for evaluation\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in valDataLoader:\n",
    "            # send the input to the device\n",
    "#             (x, y) = (x.to(device), y.to(device))\n",
    "            x = x.float().cuda(non_blocking = True)\n",
    "            y = y.view(-1).cuda(non_blocking = True)\n",
    "        \n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            totalValLoss += criterion(pred, y)\n",
    "            \n",
    "            # calculate the number of correct predictions\n",
    "            valCorrect += (pred.argmax(1) == y).type(torch.float).sum().item() \n",
    "        \n",
    "    torch.save(model.state_dict(), f'/home/yoson/SparseInst/official/SparseInst/table-tennis/pose_data/model/modelS_splitorder_pose_model_allframe_{idx}.pth')\n",
    "        \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    # calculate the training and validation accuracy\n",
    "    trainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
    "    valCorrect = valCorrect / len(valDataLoader.dataset)\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(trainCorrect)\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(valCorrect)\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "        avgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "        avgValLoss, valCorrect))\n",
    "        \n",
    "        \n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376620dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy8x4194304 \n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(args[\"plot\"])\n",
    "# serialize the model to disk\n",
    "# torch.save(model, args[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f2f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/yoson/SparseInst/official/SparseInst/table-tennis/pose_data/model/splitorder_pose_model_allframe_1378.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad41007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3df15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in trainDataLoader:\n",
    "        # send the input to the device\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report([y[0] for x,y in train_dataset], preds, labels=[1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dd83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in valDataLoader:\n",
    "        # send the input to the device\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report([y[0] for x,y in val_dataset], preds, labels=[1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93db749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in testDataLoader:\n",
    "        # send the input to the device\n",
    "#         print(x)\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report([y[0] for x,y in test_dataset], preds, labels=[1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aad28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[y for x,y in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2b3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddec4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dba221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226398d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63862f2e",
   "metadata": {},
   "source": [
    "# Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0af80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = Conv3DNet(num_classes)\n",
    "model_eval.load_state_dict(torch.load('/home/yoson/SparseInst/official/SparseInst/table-tennis/pose_data/model/splitorder_pose_model_allframe_1377.pth'))\n",
    "model_eval.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a350c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8ebb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "ys = []\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model_eval = model_eval.cuda()\n",
    "    model_eval.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in trainDataLoader:\n",
    "#         print(np.array(x).shape)\n",
    "        # send the input to the device\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model_eval(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        ys.extend(y.cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report(ys, preds, labels=[1, 2, 3, 4, 0]))\n",
    "# print(np.array(preds).shape)\n",
    "# print(preds)\n",
    "# print(\"------------------------------\")\n",
    "# print([y[0] for x,y in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "# print(len(gt_label.reshape(gt_label.shape[0])))\n",
    "# # print(gt_label)\n",
    "# print(np.array(preds).shape)\n",
    "# print(preds)\n",
    "print(classification_report(ys, preds, labels=[1, 2, 3, 4, 0]))\n",
    "\n",
    "labels = ['1', '2', '3', '4', '0']\n",
    "cm = confusion_matrix(ys, preds)\n",
    "f = sns.heatmap(cm, annot=True, fmt='d', cmap=\"BuPu\")\n",
    "f.set_xticklabels(labels)\n",
    "f.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017398ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "\n",
    "ys_val = []\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model_eval = model_eval.cuda()\n",
    "    model_eval.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in valDataLoader:\n",
    "        # send the input to the device\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model_eval(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        ys_val.extend(y.cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report([y[0] for x,y in val_dataset], preds, labels=[1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "# print(len(gt_label.reshape(gt_label.shape[0])))\n",
    "# # print(gt_label)\n",
    "# print(np.array(preds).shape)\n",
    "# print(preds)\n",
    "print(classification_report(ys_val, preds, labels=[1, 2, 3, 4, 0]))\n",
    "\n",
    "labels = ['1', '2', '3', '4', '0']\n",
    "cm = confusion_matrix(ys_val, preds)\n",
    "f = sns.heatmap(cm, annot=True, fmt='d', cmap=\"BuPu\")\n",
    "f.set_xticklabels(labels)\n",
    "f.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aaf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "\n",
    "ys_test = []\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in testDataLoader:\n",
    "        # send the input to the device\n",
    "#         print(x)\n",
    "#         x = x.to(device)\n",
    "        x = x.float().cuda(non_blocking=True)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        ys_test.extend(y.cpu().numpy())\n",
    "        \n",
    "# generate a classification report\n",
    "print(classification_report([y[0] for x,y in test_dataset], preds, labels=[1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "# print(len(gt_label.reshape(gt_label.shape[0])))\n",
    "# # print(gt_label)\n",
    "# print(np.array(preds).shape)\n",
    "# print(preds)\n",
    "print(classification_report(ys_test, preds, labels=[1, 2, 3, 4, 0]))\n",
    "\n",
    "labels = ['1', '2', '3', '4', '0']\n",
    "cm = confusion_matrix(ys, preds)\n",
    "f = sns.heatmap(cm, annot=True, fmt='d', cmap=\"BuPu\")\n",
    "f.set_xticklabels(labels)\n",
    "f.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498594c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415e5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4323749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1eb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e9331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24860f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecdb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1e609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1607f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6eb6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f714fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   Training process begins\n",
    "train_loss_list = []\n",
    "num_epochs = EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}:', end = ' ')\n",
    "    train_loss = 0\n",
    "\n",
    "    #Iterating over the training dataset in batches\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    for (images, labels) in trainDataLoader:\n",
    "        #Extracting images and target labels for the batch being iterated\n",
    "        images = images.float().cuda(non_blocking = True)\n",
    "        labels = labels.view(-1).cuda(non_blocking = True)\n",
    "\n",
    "        #Calculating the model output and the cross entropy loss\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, labels)\n",
    "        # print(pred)\n",
    "        # print('--------------------------')\n",
    "        # print(labels)\n",
    "\n",
    "        #Updating weights according to calculated loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #Printing loss for each epoch\n",
    "    train_loss_list.append(train_loss/len(trainDataLoader))\n",
    "    print(f\"Training loss = {train_loss_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22504e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting loss for all epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_loss_list)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ef868",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "model.eval()\n",
    "  \n",
    "with torch.no_grad():\n",
    "    #Iterating over the training dataset in batches\n",
    "    for i, (images, labels) in enumerate(trainDataLoader):\n",
    "         \n",
    "            \n",
    "        images = images.float().cuda(non_blocking = True)\n",
    "        y_true = labels.to(device)\n",
    "          \n",
    "        #Calculating outputs for the batch being iterated\n",
    "        outputs = model(images)\n",
    "          \n",
    "        #Calculated prediction labels from models\n",
    "        _, y_pred = torch.max(outputs.data, 1)\n",
    "          \n",
    "        #Comparing predicted and true labels\n",
    "        test_acc += (y_pred == y_true).sum().item()\n",
    "      \n",
    "    print(f\"Test set accuracy = {100 * test_acc / len(trainDataLoader)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33348c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30589914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch off autograd for evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    # loop over the validation set\n",
    "    for (datas, labels) in valDataLoader:\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "\n",
    "\n",
    "        datas = datas.float().cuda(non_blocking = True)\n",
    "        labels = labels.view(-1).cuda(non_blocking = True)\n",
    "\n",
    "        # make the predictions and calculate the validation loss\n",
    "        pred = model(x)\n",
    "        totalValLoss += lossFn(pred, y)\n",
    "        # calculate the number of correct predictions\n",
    "        valCorrect += (pred.argmax(1) == y).type(\n",
    "            torch.float).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dfe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd492d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159067e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deba948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841f61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3907313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca1ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f84cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bac71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4f990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcce3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a549ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645d474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be9172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed8794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e144995",
   "metadata": {},
   "source": [
    "# before version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d296800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import tensorflow\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses, models\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20816f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d00117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Datasets Preprocessing\n",
    "\n",
    "dataset_path = './ball_data/pose_label/label/test'\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "batch=12\n",
    "window_size = 12\n",
    "for _, walk_item in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "    root, dirs, files = walk_item\n",
    "\n",
    "    if not dirs and files:\n",
    "\n",
    "        label_path = None\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            if \"_label.csv\" in file:\n",
    "                label_path = os.path.join(root, file)\n",
    "\n",
    "        if label_path:\n",
    "            ## \n",
    "            label_df = pd.read_csv(f'{label_path}')\n",
    "            label_df['label'] = label_df['label'].astype(int)\n",
    "\n",
    "            n = 4\n",
    "            k = batch//2\n",
    "            ## \n",
    "            for _, row in label_df.iterrows():\n",
    "                ## Frame  \n",
    "                # if row['label'] in range(1, 5):\n",
    "                #     pd_filter_1 = label_df['Person_id'] == 0\n",
    "                # elif row['label'] in range(5, 9):\n",
    "                #     pd_filter_1 = label_df['Person_id'] == 1\n",
    "\n",
    "                # if row['label'] in [1, 5]:\n",
    "                #     label = [1]\n",
    "                # elif row['label'] in [2, 6]:\n",
    "                #     label = [2]\n",
    "                # elif row['label'] in [3, 4, 7, 8]:\n",
    "                #     label = [0]\n",
    "\n",
    "\n",
    "                # if row['label'] in range(1, 5):\n",
    "                #     pd_filter_1 = label_df['Person_id'] == 0\n",
    "                # elif row['label'] in range(5, 9):\n",
    "                #     pd_filter_1 = label_df['Person_id'] == 1\n",
    "\n",
    "                if int(row['label']) in [5]:         # \n",
    "                    label = [1]\n",
    "                elif int(row['label']) in [6]:       # \n",
    "                    label = [2]\n",
    "                elif int(row['label']) in [7]:       # \n",
    "                    label = [3]\n",
    "                elif int(row['label']) in [8]:       # \n",
    "                    label = [4]\n",
    "                else:\n",
    "                    label = [0]\n",
    "\n",
    "\n",
    "                for i in range((row['end'] - row['start'] - window_size) + 1):\n",
    "                    print('group_num', (row['end'] - row['start'] - window_size) + 1)\n",
    "                    img_list = []\n",
    "                    begin = row['start'] + i\n",
    "                    for j in range (begin, begin + 12):\n",
    "                        # img_list.append(cv2.imread(f'/home/yoson/SparseInst/official/SparseInst/table-tennis/label_mask/merge_R/merge_R_frame_{j}.png'))\n",
    "                        img = cv2.imread(f'./label_mask/merge_R_gray/merge_R_frame_{j}.png')\n",
    "#                         img = cv2.imread(f'/home/yoson/SparseInst/official/SparseInst/table-tennis/label_mask/merge_R_rgb/merge_R_frame_{j}.png')\n",
    "                        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA)    # (1080, 960) (256, 256)\n",
    "                        img_list.append(img)\n",
    "\n",
    "                        print(img.shape)\n",
    "              \n",
    "        \n",
    "                    print(\"start: \", begin)\n",
    "                    print(\"end: \", begin + 12)\n",
    "                    print(\"label: \", label)\n",
    "                    print(row['label'])\n",
    "                    print(len(img_list))\n",
    "                    img_list_np = np.asarray(img_list)\n",
    "                    print(img_list_np.shape)\n",
    "\n",
    "                    train_data.append(img_list)\n",
    "                    train_label.append(label)\n",
    "\n",
    "            # train_data.append(\"/home/yoson/SparseInst/official/SparseInst/table-tennis/label_mask/merge_R/\", )\n",
    "            # train_label.append(label)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "# np_train_data = np.asarray(train_data).reshape(-1, window_size, 256, 256, 3)\n",
    "np_train_data = np.asarray(train_data)\n",
    "print(np_train_data.shape)\n",
    "np_train_label = np.asarray(train_label).reshape(-1, 1)\n",
    "print(np_train_label.shape)\n",
    "print(np_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c044ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_data.shape, np_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_data.shape\n",
    "# print(data2[0])\n",
    "# np.min(data2[0])\n",
    "# plt.imshow(data2[0])\n",
    "# cv2.imshow(np_train_data[0])\n",
    "# plt.imshow(cv2.cvtColor(np_train_data[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bb006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c9509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54822c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv3D(64, 3, padding='same',activation='relu', input_shape=(12, 256, 256, 3), data_format='channels_first'))\n",
    "\n",
    "model.add(layers.MaxPooling3D(2))\n",
    "\n",
    "model.add(layers.Conv3D(128, 3, padding='same', activation='relu', data_format='channels_first'))\n",
    "\n",
    "model.add(layers.MaxPooling3D(2))\n",
    "\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Conv3D(256, 3, padding='same', activation='relu', data_format='channels_first'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59759a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd14d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, 3, padding='same',activation='relu', input_shape=(12, 512, 512), data_format='channels_first'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.Conv2D(128, 3, padding='same', activation='relu', data_format='channels_first'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(256, 3, padding='same', activation='relu', data_format='channels_first'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f79884",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu', input_shape=(12, 512, 512, 1), data_format='channels_last'))\n",
    "model.add(layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(128, (3, 3, 3), padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(256, (3, 3, 3), padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    \n",
    "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "    \n",
    "    loss = tensorflow.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    metrics = [tensorflow.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e730f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e57761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(np_train_data, np_train_label, epochs = 30, batch_size = 8, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db391b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'], label='sparse_categorical_accuracy')\n",
    "\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'val_sparse_categorical_accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(np_train_data,  np_train_label, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e666a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf513ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb827b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17dae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034d039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e5142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "from torch.utils.data import random_split\n",
    "# set the numpy seed for better reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# import the necessary packages\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import KMNIST\n",
    "import argparse\n",
    "import imutils\n",
    "import torch\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=3, padding='same')\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding='same')\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3,  padding='same')\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 3 * 64 * 64, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f'x  : {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        print(f'x maxpool1 : {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        print(f'x shape : {x.shape}')\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        output  = self.softmax(x)\n",
    "        \n",
    "        return output \n",
    "\n",
    "num_classes = 5\n",
    "model = Conv3DNet(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b65864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Conv2DNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 64 * 64, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "num_classes = 5\n",
    "model = Conv2DNet(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555695af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65874d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b658dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np_train_data_reshape = np_train_data.transpose((0,4,1,2,3))\n",
    "np_train_data_reshape = np_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de104931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_train_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np_train_data_reshape:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b503c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375021cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-4\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 100\n",
    "\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "for i in range(len(np_train_data_reshape)):\n",
    "#     print(np_train_data_reshape[i].shape())\n",
    "    train_data_list.append([np_train_data_reshape[i], np_train_label[i]])\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(train_data_list, shuffle=True, batch_size=BATCH_SIZE)\n",
    "# i1, l1 = next(iter(trainloader))\n",
    "# print(i1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca317378",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_data_list\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "i1, l1 = next(iter(train_dataset))\n",
    "print(len(train_dataset))\n",
    "print(i1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073aaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "# testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e7341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd40c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the Conv3DNet model\n",
    "print(\"[INFO] initializing the Conv3DNet model...\")\n",
    "num_classes = 5\n",
    "# model = Conv3DNet(num_classes)\n",
    "model = Conv2DNet(num_classes)\n",
    "print(model)\n",
    "# print(model)\n",
    "\n",
    "# initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacd50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = model.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9801f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967ceab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777a370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "model = model.cuda()\n",
    "# model = model.to(device)\n",
    "loss = loss.to(device)\n",
    "\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize the number of correct predictions in the training\n",
    "    # and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for (x, y) in trainDataLoader:\n",
    "        # send the input to the device\n",
    "#         (x, y) = (x.to(device), y.to(device))\n",
    "#         (x, y) = (x.to(device).type(torch.FloatTensor), y.to(device))\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "        print(x.device)\n",
    "        exit()\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "        print(pred)\n",
    "        print('---------')\n",
    "        print(y)\n",
    "        exit()\n",
    "        loss = loss(pred, y)\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # add the loss to the total training loss so far and\n",
    "        # calculate the number of correct predictions\n",
    "        totalTrainLoss += loss\n",
    "        trainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d02c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 1000, (100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb204ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Device Name: ' , x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87275d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Device Name after transferring: ', x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e85eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize the number of correct predictions in the training\n",
    "    # and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for (x, y) in trainDataLoader:\n",
    "        # send the input and target tensors to the device\n",
    "        (x, y) = (x.to(device).type(torch.FloatTensor), y.to(device))\n",
    "        \n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "        loss = loss_function(pred, y)\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # add the loss to the total training loss so far and\n",
    "        # calculate the number of correct predictions\n",
    "        totalTrainLoss += loss.item()\n",
    "        trainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffa9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparseinst",
   "language": "python",
   "name": "sparseinst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
